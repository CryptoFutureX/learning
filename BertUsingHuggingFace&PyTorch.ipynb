{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertUsingHuggingFace&PyTorch.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybPpoOkHITFK"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIL20Sy-3qgX"
      },
      "source": [
        "# Importing Modules and Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHfQxyMdJ6EG"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEsl2tpDLbGW"
      },
      "source": [
        "!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
        "!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4BRaeRHN35M"
      },
      "source": [
        "df = pd.read_csv('traindata.csv',encoding = \"latin\")\n",
        "df2 = {'0':0,'1467810369':'1467810369','Mon Apr 06 22:19:45 PDT 2009':'Mon Apr 06 22:19:45 PDT 2009','NO_QUERY':'NO_QUERY','_TheSpecialOne_':'_TheSpecialOne_',\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\":\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"}\n",
        "df = df.append(df2, ignore_index = True) \n",
        "df.columns=['target','id','date','flag','user','text']\n",
        "df = df.drop(columns=['id','date','flag','user'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnmviiBhO8GN"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrZPf7EoLBhZ"
      },
      "source": [
        "df.iloc[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL_R9TPOPBt1"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvZUl55xPE_t"
      },
      "source": [
        "sns.countplot(df.target)\n",
        "plt.xlabel('review score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJPgArO0P1jt"
      },
      "source": [
        "def to_sentiment(rating) :\n",
        "  rating = int(rating) \n",
        "  if rating == 0 :\n",
        "    return 0\n",
        "  return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9PyOo2qQ3UE"
      },
      "source": [
        "df['sentiment'] = df['target'].apply(to_sentiment)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U7JCNK5Q-mU"
      },
      "source": [
        "class_name = ['neutral','positive']\n",
        "ax = sns.countplot(df.sentiment)\n",
        "ax.set_xticklabels(class_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkyd8g8yLkSU"
      },
      "source": [
        "df = df.drop(columns=['target'])\n",
        "df.columns = ['content', 'sentiment']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXKQN66dSM_H"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rimpj01_RINk"
      },
      "source": [
        "# tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR7ov9PZzUjO"
      },
      "source": [
        "sample_text = 'When was i last outside? I am stuck at home for 2 weeks.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iON9rtV82ey3"
      },
      "source": [
        "##Word Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSkqmRf42ZNJ"
      },
      "source": [
        "tokens = tokenizer.tokenize(sample_text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlbxoW0H2dWX"
      },
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"{sample_text} \\n{tokens} \\n{token_ids}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8DUG1Yc3wry"
      },
      "source": [
        "##Special Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCGfDVMe4eiK"
      },
      "source": [
        " A special token separating two different sentences in the same input\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p04CVFGy2w4H"
      },
      "source": [
        "tokenizer.sep_token , tokenizer.sep_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7InMOXKG49dn"
      },
      "source": [
        "A special token representing the class of the input "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGglgCza4DO_"
      },
      "source": [
        "tokenizer.cls_token , tokenizer.cls_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnWUmFff5TZY"
      },
      "source": [
        "A special token used to make arrays of tokens the same size for batching purpose\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ty1IMqf4O7W"
      },
      "source": [
        "tokenizer.pad_token , tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHoK4jnI55vp"
      },
      "source": [
        "A special token representing an out-of-vocabulary token\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc0btlbO5YQ5"
      },
      "source": [
        "tokenizer.unk_token , tokenizer.unk_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxITNZwO52eG"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "    sample_text,\n",
        "    max_length=32,\n",
        "    add_special_tokens = True,\n",
        "    padding='max_length',\n",
        "    return_attention_mask=True,\n",
        "    return_token_type_ids=False,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "encoding.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3s841BfTU3L"
      },
      "source": [
        "encoding['input_ids'],len(encoding['input_ids'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT0xzIS9z4o1"
      },
      "source": [
        "encoding['attention_mask'],len(encoding['attention_mask'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS2ALJc31Wk3"
      },
      "source": [
        "## Choosing sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fMOf58dz88l"
      },
      "source": [
        "token_lens = []\n",
        "for txt in df.content :\n",
        "  tokens = tokenizer.encode(txt,max_length=512,truncation=True)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfHMNwnK1sQW"
      },
      "source": [
        "sns.displot(token_lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SR_UcP5Llum"
      },
      "source": [
        "Create PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9znDll8r1zpV"
      },
      "source": [
        "class GPReviewData(Dataset) :\n",
        "  def __init__(self,reviews,targets,tokenizer,max_len) :\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self,item) :\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      max_length=self.max_len,\n",
        "      add_special_tokens = True,\n",
        "      truncation=True,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=False,\n",
        "      return_tensors='pt')\n",
        "    return {\n",
        "        'review_text':review,\n",
        "      'input_ids':encoding['input_ids'].flatten(),\n",
        "      'attention_mask':encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target,dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmAABI0gNBCg"
      },
      "source": [
        "MAX_LEN = 280\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epc2VzBxNhoG"
      },
      "source": [
        "df_train,df_test = train_test_split(df,test_size=0.1,random_state=RANDOM_SEED)\n",
        "df_val,df_test = train_test_split(df_test,test_size=0.5,random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jOn8x0jN2Tn"
      },
      "source": [
        "df_train.shape,df_val.shape,df_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hgn7veVODuG"
      },
      "source": [
        "def create_data_loader(df,tokenizer,max_len,batch_size) :\n",
        "  ds = GPReviewData(\n",
        "      reviews=df.content.to_numpy(),\n",
        "      targets=df.sentiment.to_numpy(),\n",
        "      tokenizer=tokenizer,\n",
        "      max_len=max_len\n",
        "    )\n",
        "  \n",
        "  return DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=2\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1JDUV0bO4Jx"
      },
      "source": [
        "train_data_loader = create_data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test,tokenizer,MAX_LEN,BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkiYeEsZPC3W"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50t35qccPoRW"
      },
      "source": [
        "data['input_ids'].shape,data['attention_mask'].shape,data['targets'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzlDQ2zmQESd"
      },
      "source": [
        "#Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kSlokE6sTC2"
      },
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsMNJ9ad2jti"
      },
      "source": [
        "x = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ_StFtA3B1a"
      },
      "source": [
        "x['last_hidden_state'].shape , x['pooler_output'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8x65-j5t2a"
      },
      "source": [
        "**Building Sentimental Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhqOnQpF43OK"
      },
      "source": [
        "class SentimentClassifier(nn.Module) :\n",
        "  def __init__(self,n_classes) :\n",
        "    super(SentimentClassifier,self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size,n_classes)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self,input_ids,attention_mask) :\n",
        "    x = self.bert(\n",
        "        input_ids = input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    pooled_output = x['pooler_output']\n",
        "    output = self.drop(pooled_output)\n",
        "    output = self.out(output)\n",
        "\n",
        "    return self.softmax(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONZx7bPc5I_x"
      },
      "source": [
        "model = SentimentClassifier(len(class_name))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoI1C8lmSinl"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape)\n",
        "print(attention_mask.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxQuNrKKUNP_"
      },
      "source": [
        "model(input_ids,attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TomwPCwJzH1L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRFKkQibUkYU"
      },
      "source": [
        "###Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA7TbvAmUYqF"
      },
      "source": [
        "EPOCHS = 10\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTEcRViI-rZV"
      },
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHFK9ztQBM-e"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqla_k8cGulF"
      },
      "source": [
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QggpWd78uoLz"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtJWoH7dFTTs"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGokCtLGTHSo"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(outputs)\n",
        "      real_values.extend(targets)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw8cNu6NTJxE"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFFWWcnrFfj1"
      },
      "source": [
        "class_name = class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z99h5VXSTKgP"
      },
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5zi_M9TTLyp"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3kuv9qUTNNe"
      },
      "source": [
        "idx = 2\n",
        "review_text = y_review_texts[idx]\n",
        "true_sentiment = y_test[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})\n",
        "print(\"\\n\".join(wrap(review_text)))\n",
        "print()\n",
        "print(f'True sentiment: {class_names[true_sentiment]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2icmfIjTQu1"
      },
      "source": [
        "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw9Q3ed4TSre"
      },
      "source": [
        "review_text = \"I love completing my todos! Best app ever!!!\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8SKofzeTU9J"
      },
      "source": [
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD1bKgqHTWY9"
      },
      "source": [
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "print(f'Review text: {review_text}')\n",
        "print(f'Sentiment  : {class_names[prediction]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANZDkB8JTYDU"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1gZXzqhU1LA"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MhVx3yGm_g1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}